{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e658180-c417-49c8-94a5-3770e2c633ea",
   "metadata": {
    "id": "CJMA88F7g0DB"
   },
   "source": [
    "# Variational Autoencoder: _Functional API_ with MNIST & TensorFlow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d9024e-4377-4f96-8302-4dda7b020c70",
   "metadata": {
    "id": "UGx4iaibg5R5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ReLU, LeakyReLU, Dense, Flatten, Reshape, Input, InputLayer, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Conv2DTranspose, Reshape, Lambda, Activation\n",
    "from tensorflow.keras import models, layers, datasets\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6f0c0-00c9-41f1-9d6b-d429a7a93b90",
   "metadata": {
    "id": "EtfRvu81hD_j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e211724e-7bcd-41f0-926c-5bbf94f7e109",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQ8IYYVahFgb",
    "outputId": "17bddf7d-e348-4304-c1c7-7fb52e08ffa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"TF version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef3b01a-4065-4186-bb60-a7f63371170e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzZKog6RhHDK",
    "outputId": "e2867f50-17f9-4035-bdea-f5b9ffe948f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: []\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availibility-\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU: {gpu_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765e8d76-255e-46f7-b589-534d23517e1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHYgjND4hK9U",
    "outputId": "cd94e86c-fde2-46a0-bcd1-3486a55413f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "if gpu_devices:\n",
    "    print(f\"GPU: {gpu_devices}\")\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    print(f\"GPU details: {details.get('device_name', 'Unknown GPU')}\")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a34bc-1816-439d-9ef6-b136f4cdbcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440be1c-1ab1-4e36-89c6-06b0f7efc8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34b3c67-aef4-41f6-ab15-27cdde51b837",
   "metadata": {
    "id": "QL1mDkVVjTAx"
   },
   "source": [
    "#### Data preprocessing and cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987b738a-373e-460b-aee6-551cf3ac2082",
   "metadata": {
    "id": "BEentBDDhPIW"
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa9d70c-b670-4138-989f-a39d1fb7475e",
   "metadata": {
    "id": "evxspQgzhQA2"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset-\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee184ea0-af2c-4e78-a468-d2c41e4587f4",
   "metadata": {
    "id": "Ycc0eu2yhRUO"
   },
   "outputs": [],
   "source": [
    "if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3413346-0d66-4390-9a20-6c4ba19ddb5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0voQRjEhT5i",
    "outputId": "1cf4fa9b-a80f-41a7-b91f-fce4465f5743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_shape to be used: (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\ninput_shape to be used: {input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d049f-e6ca-47fa-904f-0c82e3e56e06",
   "metadata": {
    "id": "NCtxc41qhVOw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea85b32e-6539-45a6-a558-13efc6abe7fd",
   "metadata": {
    "id": "y1LXMmcphIfe"
   },
   "outputs": [],
   "source": [
    "# Specify hyper-parameters-\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666d08a-f236-4b53-9b9c-9ac6d081bc27",
   "metadata": {
    "id": "9XiXsntMhWpl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aaffa5f-aad7-4557-90b7-08774c161a70",
   "metadata": {
    "id": "YQckXNy4hfOn"
   },
   "outputs": [],
   "source": [
    "# Convert datasets to floating point types-\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aba612-51d9-481a-9bb3-01219269f447",
   "metadata": {
    "id": "LCY_MPG8hiDG"
   },
   "source": [
    "By default the image data consists of integers between 0 and 255 for each pixel channel. Neural networks work best when each input is inside the range –1 to 1, so we need to divide by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b47aaf-d8ff-4fc1-b0c1-adc061581ce4",
   "metadata": {
    "id": "OxapTmOdhlr5"
   },
   "outputs": [],
   "source": [
    "# Normalize the training and testing datasets-\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e93b7ab-1b72-4aa0-996d-db8a99f6a5e9",
   "metadata": {
    "id": "Vhlx2T_uhnUT"
   },
   "outputs": [],
   "source": [
    "# convert class vectors/target to binary class matrices or one-hot encoded values-\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f37d77f-6304-485e-bc15-6098e7f775af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ3g4veChp_b",
    "outputId": "bcdd5a61-92d6-4b1b-cd50-f1e363df5cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (60000, 28, 28, 1), y_train.shape: (60000, 10)\n",
      "X_test.shape: (10000, 28, 28, 1), y_test.shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}\")\n",
    "print(f\"X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb2149e-8644-453c-923f-3cd0836dd263",
   "metadata": {
    "id": "iVk7EmX7hrIm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864c58a-85b2-4e26-b9c2-39692cade5f7",
   "metadata": {
    "id": "bNqjDbVJhtEJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c4ea75e-60d8-4196-b6f8-e25890de5368",
   "metadata": {
    "id": "MQWFT9oyhtOC"
   },
   "source": [
    "### Define Variational Autoencoder using _Functional API_ & _Convolutional_ layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b23416-bdee-40a7-a0c7-66f760374726",
   "metadata": {
    "id": "YME8yQznhzkW"
   },
   "outputs": [],
   "source": [
    "# Specify latent space dimensions-\n",
    "latent_space_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a35808-94c6-4e5c-8a03-83ec89bda77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecbdafc6-4092-4cec-ad54-a549dd19ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    def __init__(self, latent_dim):\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define encoder-\n",
    "        encoder_input = Input(shape = (28, 28, 1))\n",
    "\n",
    "        x = Conv2D(\n",
    "            filters = 32, kernel_size = 3,\n",
    "            strides = 2, padding = 'same')(encoder_input)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        x = Conv2D(\n",
    "            filters = 64, kernel_size = 3,\n",
    "            strides = 2, padding = 'same')(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        x = Conv2D(\n",
    "            filters = 64, kernel_size = 3,\n",
    "            strides = 1, padding = 'same')(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        x = Conv2D(\n",
    "            filters = 64, kernel_size = 3,\n",
    "            strides = 1, padding = 'same')(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        shape_before_flattening = K.int_shape(x)[1:]\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        # Instead of connecting the flattened layer directly to the 3-D latent space, we connect\n",
    "        # it to layers 'mu' and 'log_var'-\n",
    "        self.mu = Dense(units = self.latent_dim)(x)\n",
    "        self.log_var = Dense(units = self.latent_dim)(x)\n",
    "\n",
    "        # The Keras model that outputs the values of 'mu' & 'log_var' for a given input image-\n",
    "        self.encoder_mu_log = Model(encoder_input, (self.mu, self.log_var))\n",
    "        \n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            epsilon = K.random_normal(shape = K.shape(mu), mean = 0.0, stddev = 1.0)\n",
    "            return mu + K.exp(log_var / 2) * epsilon\n",
    "\n",
    "        # This Lambda layer samples a point 'z' in the latent space from the normal distribution\n",
    "        # defined by the parameters 'mu' and 'log_var'-\n",
    "        encoder_output = Lambda(sampling, name = 'encoder_output')([self.mu, self.log_var])\n",
    "\n",
    "        self.encoder = Model(encoder_input, encoder_output)\n",
    "        \n",
    "        \n",
    "        # Define decoder-\n",
    "        decoder_input = Input(shape = (self.latent_dim))\n",
    "        \n",
    "        x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "        x = Reshape(shape_before_flattening)(x)\n",
    "\n",
    "        x = Conv2DTranspose(\n",
    "            filters = 64, kernel_size = (3, 3),\n",
    "            strides = (1, 1), padding = 'same')(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        x = Conv2DTranspose(\n",
    "            filters = 64, kernel_size = (3, 3),\n",
    "            strides = (2, 2), padding = 'same')(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        x = Conv2DTranspose(\n",
    "            filters = 32, kernel_size = (3, 3),\n",
    "            strides = (2, 2), padding = 'same')(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        x = Conv2DTranspose(\n",
    "            filters = 1, kernel_size = (3, 3),\n",
    "            strides = (1, 1), padding = 'same')(x)\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "        decoder_output = x\n",
    "\n",
    "        self.decoder = Model(decoder_input, decoder_output)\n",
    "        \n",
    "   \n",
    "        # The complete autoencoder-\n",
    "\n",
    "        # The input to the autoencoder is the same as the input to the encoder.\n",
    "        model_input = encoder_input\n",
    "\n",
    "        # The output from the autoencoder is the output from the encoder passed through\n",
    "        # the decoder.\n",
    "        model_output = self.decoder(encoder_output)\n",
    "\n",
    "        # The Keras model that defines the full autoencoder — a model that takes an image,\n",
    "        # and passes it through the encoder and back out through the decoder to generate\n",
    "        # a reconstruction of the original image.\n",
    "        self.model = Model(model_input, model_output)\n",
    "\n",
    "        \n",
    "    def compile(self, learning_rate, r_loss_factor):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        def vae_r_loss(y_true, y_pred):\n",
    "            r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "            return r_loss_factor * r_loss\n",
    "\n",
    "        def vae_kl_loss(y_true, y_pred):\n",
    "            kl_loss =  -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis = 1)\n",
    "            return kl_loss\n",
    "\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            r_loss = vae_r_loss(y_true, y_pred)\n",
    "            kl_loss = vae_kl_loss(y_true, y_pred)\n",
    "            return  r_loss + kl_loss\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "            loss = vae_loss,\n",
    "            metrics = [vae_r_loss, vae_kl_loss]\n",
    "        )\n",
    "\n",
    "        return self.model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97152dee-bb71-40d1-b7a3-9cfb605e5cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ebccce9-7cb3-4843-b07b-d98dd2ba0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_dim = latent_space_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efef72e8-9739-4cbe-91c9-aa188da435c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.compile(learning_rate = 0.003, r_loss_factor = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576056d-1aeb-47d6-8ad5-13d4677f9e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0de1f6c-d304-4f78-89cb-ac584bb4f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 14, 14, 32)   320         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 14, 14, 32)   0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 7, 7, 64)     18496       leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 7, 7, 64)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 7, 7, 64)     36928       leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 7, 7, 64)     0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 7, 7, 64)     36928       leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 7, 7, 64)     0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 3136)         0           leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 3)            9411        flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 3)            9411        flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 3)            0           dense_18[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_21 (Functional)           (None, 28, 28, 1)    105153      encoder_output[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 216,647\n",
      "Trainable params: 216,647\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a2ac7-cfb0-447f-826e-9a8d8769f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55a2f633-ae1d-4883-b157-8f813acb5769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 28, 28, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:2, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d592b-82c5-496d-9bbc-1f785ed6df93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81982ebd-9471-4ed3-aac7-6b3ec6a11261",
   "metadata": {
    "id": "ecd15839-2dab-4019-bee9-5e56b890db4a"
   },
   "outputs": [],
   "source": [
    "# Define early stopping criterion-\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', min_delta = 0.0001,\n",
    "    patience = 4,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "# Adjust 'min_delta' according to training loss for early stopping\n",
    "# to effectively happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b5744-a220-4768-9296-3eb40ef5ae56",
   "metadata": {
    "id": "2ec9360b-ccb8-400e-a4c0-70378012fce9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4bebf6c3-e515-448c-9bb6-fd6393a68fff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56d889ee-bb59-421c-bf79-d34476d66b9a",
    "outputId": "3021abdc-5068-4aa9-fa55-219a8c414e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training epochs = 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of training epochs = {num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b832970-d074-4a1c-89c1-136b7596297c",
   "metadata": {
    "id": "cc61aea2-910a-43d7-95f4-85ac3792111f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1c78b-2412-4100-861d-f5992cc3ae19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04fd2fdd-70dd-4f07-87bb-46ac42b62bba",
    "outputId": "4bd83eed-1c0b-4d85-dc1e-b5b072dabb7f"
   },
   "outputs": [],
   "source": [
    "# Train autoencoder-\n",
    "training_hist = model.fit(\n",
    "    x = X_train, y = X_train,\n",
    "    batch_size = batch_size, shuffle = True,\n",
    "    validation_data = (X_test, X_test),\n",
    "    epochs = num_epochs, callbacks = [early_stopping]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14b3e9-8b71-4e67-966f-19d9c0688306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e30d1-b3ac-4515-b417-13794f033ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5c3d764-f526-4708-9f3c-536f674d7c85",
   "metadata": {
    "id": "Gfr__HdMh3cz"
   },
   "source": [
    "The last conv layer is flattened and connected to a Dense layer of size 2, which represents our 2-D latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6074b589-cdd1-4fbc-b6b2-49aef6d0bd89",
   "metadata": {
    "id": "ZLiym-Erh5N3"
   },
   "outputs": [],
   "source": [
    "# Define encoder-\n",
    "encoder_input = Input(shape = (28, 28, 1))\n",
    "\n",
    "x = Conv2D(\n",
    "    filters = 32, kernel_size = 3,\n",
    "    strides = 2, padding = 'same')(encoder_input)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters = 64, kernel_size = 3,\n",
    "    strides = 2, padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters = 64, kernel_size = 3,\n",
    "    strides = 1, padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters = 64, kernel_size = 3,\n",
    "    strides = 1, padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# shape_before_flattening = K.int_shape(x)[1:]\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Instead of connecting the flattened layer directly to the 3-D latent space, we connect\n",
    "# it to layers 'mu' and 'log_var'-\n",
    "mu = Dense(units = latent_space_dim)(x)\n",
    "log_var = Dense(units = latent_space_dim)(x)\n",
    "\n",
    "# The Keras model that outputs the values of 'mu' & 'log_var' for a given input image-\n",
    "encoder_mu_log = Model(encoder_input, (mu, log_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed030995-8529-4019-91d2-8edfd9fccc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"shape_before_flattening: {shape_before_flattening}\")\n",
    "# shape_before_flattening: (7, 7, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfcd65-3531-4229-b09e-d05b595ea77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eb5af42-feb6-4f18-ab15-8abdbd0a3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    mu, log_var = args\n",
    "    epsilon = K.random_normal(shape = K.shape(mu), mean = 0.0, stddev = 1.0)\n",
    "    return mu + K.exp(log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b18ea0-ccf7-412d-8924-7eaf20867b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e05553f7-5e6f-415b-ac82-ddfde36a78a9",
   "metadata": {},
   "source": [
    "#### Lambda layer:\n",
    "\n",
    "A Lambda layer simple wraps any function into Keras layer. For example, the following layer squares its input:\n",
    "\n",
    "```Lambda(lambda x: x ** 2)```\n",
    "\n",
    "They are useful when you want to apply a function to a tensor that isn’t already included as one of the out-of-the-box Keras layer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da9f2b7b-93ee-4df9-9f12-99816ec643c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Lambda layer samples a point 'z' in the latent space from the normal distribution\n",
    "# defined by the parameters 'mu' and 'log_var'-\n",
    "encoder_output = Lambda(sampling)([mu, log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb0214-c759-46a0-aba1-faa589e7faec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "116ad219-b73c-4ea8-93a6-7ac047c94c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Keras model that defines the encoder — a model that takes an input image and encodes it\n",
    "# into the 2D latent space, by sampling a point from the multivariate normal distribution\n",
    "# defined by 'mu' and 'log_var'-\n",
    "encoder = Model(encoder_input, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526717e4-cbf3-4200-bb49-6f529da0e5bb",
   "metadata": {
    "id": "1vIoyT29iAev"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d2715a-7f30-4a70-a867-c43779188b93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aX3S_3nnh-V4",
    "outputId": "a9bcaed8-5916-46f9-b3d5-5f8d182f64f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 14, 14, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 14, 14, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 64)     18496       leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 7, 7, 64)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 64)     36928       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 7, 7, 64)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 64)     36928       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 7, 7, 64)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3136)         0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            9411        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            9411        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 3)            0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,494\n",
      "Trainable params: 111,494\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sanity check-\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ef8bf-52b8-4995-bf54-42c44fbaafdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7dc6508-4047-4c85-bb3c-b2337354be75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXXoJTy64Lae",
    "outputId": "889e6849-c9a4-4a6a-cc4d-6832c23b11a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3, 1), dtype=float32, numpy=\n",
       "array([[[ 0.00384198],\n",
       "        [-0.03363007],\n",
       "        [-0.05510987]],\n",
       "\n",
       "       [[ 0.01851153],\n",
       "        [-0.02809589],\n",
       "        [-0.12507896]],\n",
       "\n",
       "       [[-0.08668809],\n",
       "        [ 0.04580538],\n",
       "        [ 0.1180629 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print randomly initialized weights of first\n",
    "# filter of first conv layer-\n",
    "encoder.weights[0][:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b8ef4-14b2-4879-be8b-52455f1fd496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9022c647-5526-4d06-8077-f871618b865e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Further sanity check-\n",
    "encoder(X_train[:2, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af187bee-6272-48c0-beca-111e50d2e9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output for latent space = 3:\n",
      "[[ 1.7884558  -0.26782405 -0.92898864]\n",
      " [-2.579872    1.31416     0.49726126]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encoder output for latent space = {latent_space_dim}:\\n{encoder(X_train[:2, :]).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2034ac-2b4a-41fc-a40e-0d386c8166d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90aaf2-29d4-4e39-bf0f-919a8af7d4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33ea2d-ceca-4a22-930f-09c073555334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f91bb80-7a3e-4900-bcbb-c158e0b8075c",
   "metadata": {
    "id": "5w-EJa8YiD5c"
   },
   "source": [
    "#### Define decoder-\n",
    "The decoder is a mirror image of the encoder, except instead of convolutional layers, we use convolutional transpose layers.\n",
    "Note that the decoder doesn’t have to be a mirror image of the encoder. It can be anything you want, as long as the output from the last layer of the decoder is the same size as the input to the encoder (since our loss function will be comparing these pixelwise).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d1497-b99e-4069-bf85-831516874b4d",
   "metadata": {},
   "source": [
    "- __The decoder of a variational autoencoder is identical to the decoder of a plain autoencoder__.\n",
    "\n",
    "- __The only other part we need to change is the loss function__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8dc4bf-ae9b-45c2-98f9-0b216c2607d6",
   "metadata": {
    "id": "MBbQ4ljxiM3Y"
   },
   "source": [
    "#### Convolutional Transpose Layers:\n",
    "\n",
    "Standard convolutional layers allow us to halve the size of an input tensor in both height and width, by setting strides = 2.\n",
    "\n",
    "The convolutional transpose layer uses the same principle as a standard convolutional layer (passing a filter across the image), but is different in that setting strides = 2 'doubles' the size of the input tensor in both height and width.\n",
    "\n",
    "In a convolutional transpose layer, the 'strides' parameter determines the internal zero padding between pixels in the image.\n",
    "\n",
    "In Keras, the 'Conv2DTranspose' layer allows us to perform convolutional transpose operations on tensors. By stacking these layers, we can gradually expand the size of each layer, using strides = 2, until we get back to the original image dimension of\n",
    "28 × 28.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6388f349-2412-4815-8561-6f11a89b4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape = (latent_space_dim))\n",
    "\n",
    "x = Dense(np.prod((7, 7, 64)))(decoder_input)\n",
    "x = Reshape((7, 7, 64))(x)\n",
    "\n",
    "x = Conv2DTranspose(\n",
    "    filters = 64, kernel_size = (3, 3),\n",
    "    strides = (1, 1), padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2DTranspose(\n",
    "    filters = 64, kernel_size = (3, 3),\n",
    "    strides = (2, 2), padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2DTranspose(\n",
    "    filters = 32, kernel_size = (3, 3),\n",
    "    strides = (2, 2), padding = 'same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2DTranspose(\n",
    "    filters = 1, kernel_size = (3, 3),\n",
    "    strides = (1, 1), padding = 'same')(x)\n",
    "x = Activation('sigmoid')(x)\n",
    "\n",
    "decoder_output = x\n",
    "\n",
    "decoder = Model(decoder_input, decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b86f5e-3456-4b20-9df5-5347d41092f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edec5853-c9bd-4ae2-b3dd-73f77d617f2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrhkTZdGh1AA",
    "outputId": "d241db36-5b45-4407-af40-dfce60554ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 105,153\n",
      "Trainable params: 105,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sanity check-\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced25666-0be5-4cb7-8a8a-122dc25be3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e342bdf6-d284-431f-b9bc-9f68fb5ec0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output using encoder output for latent space = 3:\n",
      "(2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check-\n",
    "print(f\"Decoder output using encoder output for latent space = {latent_space_dim}:\\n\"\n",
    "      f\"{decoder(encoder(X_train[:2, :])).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19374a44-99aa-4627-95ae-5859d4f02912",
   "metadata": {
    "id": "EII-CJKdiagq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd630ac2-53a2-487d-a3a0-dfbdf1a79690",
   "metadata": {
    "id": "Tl4ANV-YimZ6"
   },
   "source": [
    "#### Joining the Encoder to the Decoder\n",
    "To train the encoder and decoder simultaneously, we need to define a model that will represent the flow of an image through the encoder and back out through the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b048c772-e278-4c28-9d86-82c605bc160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The complete autoencoder-\n",
    "\n",
    "# The input to the autoencoder is the same as the input to the encoder.\n",
    "model_input = encoder_input\n",
    "\n",
    "# The output from the autoencoder is the output from the encoder passed through\n",
    "# the decoder.\n",
    "model_output = decoder(encoder_output)\n",
    "\n",
    "# The Keras model that defines the full autoencoder—a model that takes an image,\n",
    "# and passes it through the encoder and back out through the decoder to generate\n",
    "# a reconstruction of the original image.\n",
    "model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4cb1ed-23bd-4ab2-9050-b675352f9d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b45d2-1cf0-431b-9613-2aed2fc153d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d383db33-851b-4a51-ab15-f6d1e843946a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0dhzdrrix1j",
    "outputId": "e6e566ed-83ab-4f7f-a4a1-4c3798deb293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 14, 14, 32)   320         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 14, 14, 32)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 7, 7, 64)     18496       leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 7, 7, 64)     0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 7, 7, 64)     36928       leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 7, 7, 64)     0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 7, 7, 64)     36928       leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 7, 7, 64)     0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 3136)         0           leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 3)            9411        flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 3)            9411        flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 3)            0           dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Functional)            (None, 28, 28, 1)    105153      encoder_output[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 216,647\n",
      "Trainable params: 216,647\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Final sanity check-\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28500d05-e484-4bdb-83dc-6d4b4e130306",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxXK7c7oj7qk",
    "outputId": "329fa9c0-502c-48fd-bd72-813151d95300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param.shape: (3, 3, 1, 32) has 288 params\n",
      "param.shape: (32,) has 0 params\n",
      "param.shape: (3, 3, 32, 64) has 18432 params\n",
      "param.shape: (64,) has 0 params\n",
      "param.shape: (3, 3, 64, 64) has 36864 params\n",
      "param.shape: (64,) has 0 params\n",
      "param.shape: (3, 3, 64, 64) has 36864 params\n",
      "param.shape: (64,) has 0 params\n",
      "param.shape: (3136, 3) has 9408 params\n",
      "param.shape: (3,) has 0 params\n",
      "param.shape: (3136, 3) has 9408 params\n",
      "param.shape: (3,) has 0 params\n",
      "param.shape: (3, 3136) has 9408 params\n",
      "param.shape: (3136,) has 0 params\n",
      "param.shape: (3, 3, 64, 64) has 36864 params\n",
      "param.shape: (64,) has 0 params\n",
      "param.shape: (3, 3, 64, 64) has 36864 params\n",
      "param.shape: (64,) has 0 params\n",
      "param.shape: (3, 3, 32, 64) has 18432 params\n",
      "param.shape: (32,) has 0 params\n",
      "param.shape: (3, 3, 1, 32) has 288 params\n",
      "param.shape: (1,) has 0 params\n"
     ]
    }
   ],
   "source": [
    "# Loop through each trainable layer and print it's shape-\n",
    "tot_params = 0\n",
    "for layer in model.weights:\n",
    "    loc_param = tf.math.count_nonzero(layer, axis = None).numpy()\n",
    "    tot_params += loc_param\n",
    "    print(f\"param.shape: {layer.shape} has {loc_param} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc0f01fa-09d3-499c-97f3-62fb63147bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters in VAE model = 213120\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of trainable parameters in VAE model = {tot_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5619003-5112-439f-a472-eb0def099990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61b9b101-e547-422b-ad23-2de78dc0eea4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6T2UnzxizDc",
    "outputId": "d51b3f57-e205-4a44-da2e-377e510bc14a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 28, 28, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "model(X_train[:2, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dec5f-508d-4603-bd8a-67896b3b4ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6efa01-62c1-4b75-8323-d96f6d6f328e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d040a47e-7565-4b85-b2c0-162287068748",
   "metadata": {},
   "source": [
    "### VAE _loss_ function:\n",
    "\n",
    "Previously, our loss function only consisted of the RMSE loss between images and their reconstruction after being passed through the encoder and decoder. This reconstruction loss also appears in a variational autoencoder, but we require one extra component: The Kullback–Leibler (KL) divergence.\n",
    "\n",
    "- KL divergence is a way of measuring how much one probability distribution differs from another.\n",
    "- In a VAE, we want to measure how different our normal distribution with parameters mu and log_var is from the standard normal distribution.\n",
    "- The sum is taken over all the dimensions in the latent space.\n",
    "- KL loss is minimized to 0 when mu = 0 and log_var = 0 for all dimensions. As these two terms start to differ from 0, KL loss increases.\n",
    "- In summary, the KL divergence term penalizes the network for encoding observations to mu and log_var variables that differ significantly from the parameters of a standard normal distribution, namely mu = 0 and log_var = 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0411b81-83a9-4e89-bbf7-36a414f1dc2e",
   "metadata": {},
   "source": [
    "#### Why does this addition to the loss function help?\n",
    "\n",
    "1. First, we now have a well-defined distribution that we can use for choosing points in the latent space—the standard normal distribution. If we sample from this distribution, we know that we’re very likely to get a point that lies within the limits of what the VAE is used to seeing.\n",
    "\n",
    "1. Secondly, since this term (KL divergence) tries to force all encoded distributions toward the standard normal distribution, there is less chance that large gaps will form between point clusters. Instead, the encoder will try to use the space around the origin symmetrically and efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653bb7b-a518-48a8-b3d3-16e83f6f526f",
   "metadata": {},
   "source": [
    "- In the code, the loss function for a VAE is simply the addition of the reconstruction loss and the KL divergence loss term; loss = reconstruction loss + KL divergence loss.\n",
    "\n",
    "- We weight the reconstruction loss with a term, r_loss_factor, that ensures that it is well balanced with the KL divergence loss.\n",
    "\n",
    "- If we weight the reconstruction loss too heavily, the KL loss will not have the desired regulatory effect and we will see the same problems that we experienced with the plain autoencoder.\n",
    "\n",
    "- If the weighting term is too small, the KL divergence loss will dominate and the reconstructed images will be poor.\n",
    "\n",
    "- This weighting term is one of the parameters to tune when you’re training your VAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b15d1254-18e0-45af-a76e-5e76ee7d18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight the reconstruction loss 'r_loss_factor' to ensure that it is well balanced with the KL divergence loss-\n",
    "r_loss_factor = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c1e0e7b-9ff5-4f2b-9258-74d3afd922c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_r_loss(y_true, y_pred):\n",
    "    # Reconstruction loss-\n",
    "    r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "    return r_loss_factor * r_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1be72bd2-fe1a-4933-b07c-d4c107328fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_kl_loss(y_true, y_pred):\n",
    "    # KL-Divergence loss-\n",
    "    kl_loss = -0.5 * K.sum(1 + log_var - K.square(mu) - K.exp(log_var), axis = 1)\n",
    "    return kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee35f843-3b5c-445a-8f79-be3fe694b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    # VAE loss = Reconstruction loss + KL-Divergence loss\n",
    "    r_loss = vae_r_loss(y_true, y_pred)\n",
    "    kl_loss = vae_kl_loss(y_true, y_pred)\n",
    "    return r_loss + kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbad210-0cc5-43d7-bf46-0fcb7f0b5182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75824bb1-d1a8-4728-b6ba-a5f001e3ac71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'tf.math.square')>,\n",
       " <KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'tf.math.exp')>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.square(mu), K.exp(log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc4b2f-6a56-4938-9851-937cdbe6cfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f0a62da-1f83-4a72-82bc-d822583632e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model-\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.003),\n",
    "    loss = vae_loss,\n",
    "    metrics = [vae_r_loss, vae_kl_loss]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eedfa-d172-46d4-8213-346ee0b40d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9997e501-b8a7-442d-8050-8ba627dd233e",
   "metadata": {
    "id": "ecd15839-2dab-4019-bee9-5e56b890db4a"
   },
   "outputs": [],
   "source": [
    "# Define early stopping criterion-\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', min_delta = 0.0001,\n",
    "    patience = 4,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "# Adjust 'min_delta' according to training loss for early stopping\n",
    "# to effectively happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2d814-ffd0-4a9e-9918-cf4eae22c126",
   "metadata": {
    "id": "2ec9360b-ccb8-400e-a4c0-70378012fce9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9616e67b-7308-447f-9ed7-9fc61d1f3d9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56d889ee-bb59-421c-bf79-d34476d66b9a",
    "outputId": "3021abdc-5068-4aa9-fa55-219a8c414e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training epochs = 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of training epochs = {num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60ab6b-edf9-4a29-aebb-c7f825fdbc47",
   "metadata": {
    "id": "cc61aea2-910a-43d7-95f4-85ac3792111f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e881d-c862-45b7-9ec7-840bf594cbc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04fd2fdd-70dd-4f07-87bb-46ac42b62bba",
    "outputId": "4bd83eed-1c0b-4d85-dc1e-b5b072dabb7f"
   },
   "outputs": [],
   "source": [
    "# Train autoencoder-\n",
    "training_hist = model.fit(\n",
    "    x = X_train, y = X_train,\n",
    "    batch_size = batch_size, shuffle = True,\n",
    "    validation_data = (X_test, X_test),\n",
    "    epochs = num_epochs, callbacks = [early_stopping]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30acabf9-9aae-4340-a52c-d824d9adeee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
